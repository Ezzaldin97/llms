# Language Models ðŸ¤–

## Objective:

The Process of creating **Small/Large Language Model** involves different steps, each step has a target to achieve.
the objective of this repository is to uncover everything about these steps, throughout our journey we will discuss:

- **Difference between models at each step**: purpose of pretraining, instruction tuning and Perference Optimization.
- **Tokenizers**: how to create, test, and train your tokenizer.
- **Datasets**: difference between datasets in pretraining/instruction tuning/preference optimization, and how to create them. 
- **Pretraining**: how to create a pretrained(base) model suitable for your task.
- **Instruction Tuning**: creating Instruct Model.
- **Preference Optimization**:  SLM/LLM alighnment.
- **Applications**: creating Agents, and RAGs.

The journey may be long, tough, and need a lot of search, but interesting.

## Prerequisities:

- Python.
- knowledge in Deep Learning.

## Tools:

- Python
- PyTorch
- Hugging Face (Transformers/Datasets)
- Ollama

## Content:

### 1- Introduction:

The aim of this introduction is to make you understand what's the difference between a **pretrained model/base model**, **Instruct model**.

by the end of this part you should understand the following ideas:

- What's Pretraining? How to Pretrain a Model? How the data looks like in this stage? where to find it?
- Why do we need to pretrain the model again? How to efficiently pretrain a model? 
- What's Instruction Tuning? How to create an Instruct Model? How the data looks like?
- Understand The purpose between pretraining and Instruction Tuning.

check this: [introduction](introduction/)