{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "The purpose of this notebook is clarify the difference between general-purpose pretrained model, custom pretrained model, tiny Instruct model, and small Instruct model.\n",
    "\n",
    "This notebook tries to test the responses of each model on a given prompt, which is here a python code to complete.\n",
    "\n",
    "by the end of this notebook, you will understand the ability of each one, and the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\llms\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "### import packages\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "import ollama \n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for reproducibility\n",
    "def set_seed(seed = 123):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set the value of some constants\n",
    "MODEL = \"gpt2\"\n",
    "DEVICE = \"auto\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODELS_PATH = os.path.join(\"..\", \"assets\", \"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load a general pretrained model\n",
    "\n",
    "This notebook will use small models that fit within the memory and run on CPU. **gpt-2 small** is a small decoder-only model with 137M parameters and a 1024 token context window(we will discuss this concepts later). You can find the model on the Hugging Face model library at [this link](https://huggingface.co/openai-community/gpt2).\n",
    "\n",
    "You'll load the model in three steps:\n",
    "1. Specify the path to the model in the Hugging Face model library\n",
    "2. Load the model using `AutoModelforCausalLM` in the `transformers` library\n",
    "3. Load the tokenizer for the model from the same model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL\n",
    ")\n",
    "\n",
    "gpt2_small_base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    device_map=DEVICE, # change to auto if you have access to a GPU\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir = MODELS_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer.pad_token_id = gpt2_tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test the tokenizer for input python code...\n",
    "py_code = '''\n",
    "def write_map_file(mapFNH, items, header):\n",
    "    \"\"\"\n",
    "    Given a list of mapping items (in the form described by the parse_mapping_file method)\n",
    "    and a header line, write each row to the given input file with fields separated by tabs.\n",
    "\n",
    "    :type mapFNH: file or str\n",
    "    :param mapFNH: Either the full path to the map file or an open file handle\n",
    "\n",
    "    :type items: list\n",
    "    :param item: The list of row entries to be written to the mapping file\n",
    "\n",
    "    :type header: list or str\n",
    "    :param header: The descriptive column names that are required as the first line of\n",
    "                   the mapping file\n",
    "\n",
    "    :rtype: None\n",
    "    \"\"\"\n",
    "    if isinstance(header, list):\n",
    "        header = \"\\t\".join(header) + \"\\n\"\n",
    "\n",
    "    with file_handle(mapFNH, \"w\") as mapF:\n",
    "        mapF.write(header)\n",
    "        for row in items:\n",
    "            mapF.write(\"\\t\".join(row)+\"\\n\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ċ', 'def', 'Ġwrite', '_', 'map', '_', 'file', '(', 'map', 'F', 'NH', ',', 'Ġitems', ',', 'Ġheader', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ\"\"\"', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'ĠGiven', 'Ġa', 'Ġlist', 'Ġof', 'Ġmapping', 'Ġitems', 'Ġ(', 'in', 'Ġthe', 'Ġform', 'Ġdescribed', 'Ġby', 'Ġthe', 'Ġparse', '_', 'm', 'apping', '_', 'file', 'Ġmethod', ')', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġand', 'Ġa', 'Ġheader', 'Ġline', ',', 'Ġwrite', 'Ġeach', 'Ġrow', 'Ġto', 'Ġthe', 'Ġgiven', 'Ġinput', 'Ġfile', 'Ġwith', 'Ġfields', 'Ġseparated', 'Ġby', 'Ġtabs', '.', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ:', 'type', 'Ġmap', 'F', 'NH', ':', 'Ġfile', 'Ġor', 'Ġstr', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ:', 'param', 'Ġmap', 'F', 'NH', ':', 'ĠEither', 'Ġthe', 'Ġfull', 'Ġpath', 'Ġto', 'Ġthe', 'Ġmap', 'Ġfile', 'Ġor', 'Ġan', 'Ġopen', 'Ġfile', 'Ġhandle', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ:', 'type', 'Ġitems', ':', 'Ġlist', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ:', 'param', 'Ġitem', ':', 'ĠThe', 'Ġlist', 'Ġof', 'Ġrow', 'Ġentries', 'Ġto', 'Ġbe', 'Ġwritten', 'Ġto', 'Ġthe', 'Ġmapping', 'Ġfile', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ:', 'type', 'Ġheader', ':', 'Ġlist', 'Ġor', 'Ġstr', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ:', 'param', 'Ġheader', ':', 'ĠThe', 'Ġdescriptive', 'Ġcolumn', 'Ġnames', 'Ġthat', 'Ġare', 'Ġrequired', 'Ġas', 'Ġthe', 'Ġfirst', 'Ġline', 'Ġof', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġthe', 'Ġmapping', 'Ġfile', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ:', 'r', 'type', ':', 'ĠNone', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ\"\"\"', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġif', 'Ġis', 'instance', '(', 'header', ',', 'Ġlist', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġheader', 'Ġ=', 'Ġ\"', 'ĉ', '\".', 'join', '(', 'header', ')', 'Ġ+', 'Ġ\"', 'Ċ', '\"', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġwith', 'Ġfile', '_', 'handle', '(', 'map', 'F', 'NH', ',', 'Ġ\"', 'w', '\")', 'Ġas', 'Ġmap', 'F', ':', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġmap', 'F', '.', 'write', '(', 'header', ')', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġfor', 'Ġrow', 'Ġin', 'Ġitems', ':', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġmap', 'F', '.', 'write', '(\"', 'ĉ', '\".', 'join', '(', 'row', ')+', '\"', 'Ċ', '\")', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "### tokenize the given text chunk into tokens.\n",
    "### testing the tokenizer is very important, and we will discuss this later...\n",
    "py_code_tokens = gpt2_tokenizer.tokenize(py_code)\n",
    "print(py_code_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a problem in gpt2 tokenizer when tokenizing python code:\n",
    "\n",
    "- This tokenizer has a few special symbols, like Ġ and Ċ, which denote spaces and newlines\n",
    "- this is not too efficient: the tokenizer returns individual tokens for each space, when it could group together indentation levels.\n",
    "\n",
    "this can be an indication that this model doesn't train enough on python code, which indicates a weak performance on simplea autocomplete tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Python samples with pretrained general model\n",
    "\n",
    "Use the model to write a python function called `find_max()` that finds the maximum value in a list of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # This is the number of times a given string has been found. if len (strings) > 1: return False def get_string(s): \"\"\" Returns an array containing strings that are sorted by length, and returns True for all characters in it.\"\"\" s = [] while not strlen(s): print \"Invalid character\" else : try { fprintf(stderr, \"%d\n",
      "\", String.format(str)) except ValueError: raise Exception(\"Failed to parse '%i' or %u\".join() + \"\").strip().lower() elif n == 0: break } catch None,\n"
     ]
    }
   ],
   "source": [
    "prompt =  \"def find_max(numbers):\"\n",
    "\n",
    "inputs = gpt2_tokenizer(\n",
    "    prompt, return_tensors=\"pt\"\n",
    ").to(gpt2_small_base.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    gpt2_tokenizer, \n",
    "    skip_prompt=True, # Set to false to include the prompt in the output\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "outputs = gpt2_small_base.generate(\n",
    "    **inputs, \n",
    "    streamer=streamer, \n",
    "    use_cache=True, \n",
    "    max_new_tokens=128, \n",
    "    do_sample=False, \n",
    "    temperature=0.0, \n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as expected the model generates garbage, it generates python code tokens but not the ones that we need to complete the input prompt, or answer the given input.\n",
    "\n",
    "this is a sign of why we need to pretrain our model on custom data.\n",
    "\n",
    "also, if the pretrained model doesn't train on data/task similar to the one you are asking about, you must pretrain it\n",
    "example:\n",
    "\n",
    "gpt-2 small doesn't train on arabic, so if your task is to create arabic instruct model, you have to pretrain the model on arabic text chunks, then do instruction tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: We're running large models in a limited environment. Run me if you encounter any memory issues.\n",
    "import gc\n",
    "del gpt2_small_base\n",
    "del gpt2_tokenizer\n",
    "del streamer\n",
    "del outputs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Python samples with finetuned Python model\n",
    "\n",
    "This model has been fine-tuned on instruction code examples. You can find the model and information about the fine-tuning datasets on the Hugging Face model library at [this link](https://huggingface.co/upstage/TinySolar-248m-4k-code-instruct).\n",
    "\n",
    "You'll follow the same steps as above to load the model and use it to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL1 = \"upstage/TinySolar-248m-4k-code-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### code instruct model....\n",
    "code_tiny_solar_instruct = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL1,\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=torch.bfloat16,  \n",
    "    cache_dir = MODELS_PATH,  \n",
    ")\n",
    "\n",
    "code_tiny_solar_instruct_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   if len(numbers) == 0:\n",
      "       return \"Invalid input\"\n",
      "   else:\n",
      "       return max(numbers)\n",
      "```\n",
      "\n",
      "In this solution, the `find_max` function takes a list of numbers as input and returns the maximum value in that list. It then iterates through each number in the list and checks if it is greater than or equal to 1. If it is, it adds it to the `max` list. Finally, it returns the maximum value found so far.\n"
     ]
    }
   ],
   "source": [
    "inputs = code_tiny_solar_instruct_tokenizer(\n",
    "    prompt, return_tensors=\"pt\"\n",
    ").to(code_tiny_solar_instruct.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    code_tiny_solar_instruct_tokenizer,\n",
    "    skip_prompt=True, \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "outputs = code_tiny_solar_instruct.generate(\n",
    "    **inputs, streamer=streamer,\n",
    "    use_cache=True, \n",
    "    max_new_tokens=128, \n",
    "    do_sample=False, \n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see above the model generates a good answer to complete the input, and also gives a clarification about the generated code.\n",
    "\n",
    "the instruct model here gives a detailed answer, not only completing the code, but also giving a clarification.\n",
    "\n",
    "this may be the difference between instruct model and base model:\n",
    "\n",
    "- the base model: generates tokens that is suitable for the given context.\n",
    "- the instruct model: generates tokens that answers the given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: We're running large models in a limited environment. Run me if you encounter any memory issues.\n",
    "import gc\n",
    "del code_tiny_solar_instruct_tokenizer\n",
    "del code_tiny_solar_instruct\n",
    "del streamer\n",
    "del outputs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Python samples with pretrained Python model\n",
    "\n",
    "Here you'll use a version of TinySolar-248m-4k that has been further pretrained (a process called **continued pretraining**) on a large selection of python code samples. You can find the model on Hugging Face at [this link](https://huggingface.co/upstage/TinySolar-248m-4k-py).\n",
    "\n",
    "You'll follow the same steps as above to load the model and use it to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL2 = \"upstage/TinySolar-248m-4k-py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pretrained model on python code....\n",
    "py_tiny_solar = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL2,\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=torch.bfloat16,  \n",
    "    cache_dir = MODELS_PATH,  \n",
    ")\n",
    "\n",
    "py_tiny_solar_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '<0x0A>', 'def', '▁write', '_', 'map', '_', 'file', '(', 'map', 'FN', 'H', ',', '▁items', ',', '▁header', '):', '<0x0A>', '▁▁▁', '▁\"\"\"', '<0x0A>', '▁▁▁', '▁Given', '▁a', '▁list', '▁of', '▁mapping', '▁items', '▁(', 'in', '▁the', '▁form', '▁described', '▁by', '▁the', '▁parse', '_', 'mapping', '_', 'file', '▁method', ')', '<0x0A>', '▁▁▁', '▁and', '▁a', '▁header', '▁line', ',', '▁write', '▁each', '▁row', '▁to', '▁the', '▁given', '▁input', '▁file', '▁with', '▁fields', '▁separated', '▁by', '▁t', 'abs', '.', '<0x0A>', '<0x0A>', '▁▁▁', '▁:', 'type', '▁map', 'FN', 'H', ':', '▁file', '▁or', '▁str', '<0x0A>', '▁▁▁', '▁:', 'param', '▁map', 'FN', 'H', ':', '▁Either', '▁the', '▁full', '▁path', '▁to', '▁the', '▁map', '▁file', '▁or', '▁an', '▁open', '▁file', '▁handle', '<0x0A>', '<0x0A>', '▁▁▁', '▁:', 'type', '▁items', ':', '▁list', '<0x0A>', '▁▁▁', '▁:', 'param', '▁item', ':', '▁The', '▁list', '▁of', '▁row', '▁entries', '▁to', '▁be', '▁written', '▁to', '▁the', '▁mapping', '▁file', '<0x0A>', '<0x0A>', '▁▁▁', '▁:', 'type', '▁header', ':', '▁list', '▁or', '▁str', '<0x0A>', '▁▁▁', '▁:', 'param', '▁header', ':', '▁The', '▁des', 'cript', 'ive', '▁column', '▁names', '▁that', '▁are', '▁required', '▁as', '▁the', '▁first', '▁line', '▁of', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁', '▁the', '▁mapping', '▁file', '<0x0A>', '<0x0A>', '▁▁▁', '▁:', 'r', 'type', ':', '▁None', '<0x0A>', '▁▁▁', '▁\"\"\"', '<0x0A>', '▁▁▁', '▁if', '▁isinstance', '(', 'header', ',', '▁list', '):', '<0x0A>', '▁▁▁▁▁▁▁', '▁header', '▁=', '▁\"', '<0x09>', '\".', 'join', '(', 'header', ')', '▁+', '▁\"', '<0x0A>', '\"', '<0x0A>', '<0x0A>', '▁▁▁', '▁with', '▁file', '_', 'handle', '(', 'map', 'FN', 'H', ',', '▁\"', 'w', '\")', '▁as', '▁map', 'F', ':', '<0x0A>', '▁▁▁▁▁▁▁', '▁map', 'F', '.', 'write', '(', 'header', ')', '<0x0A>', '▁▁▁▁▁▁▁', '▁for', '▁row', '▁in', '▁items', ':', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁', '▁map', 'F', '.', 'write', '(\"', '<0x09>', '\".', 'join', '(', 'row', ')+', '\"', '<0x0A>', '\")', '<0x0A>']\n"
     ]
    }
   ],
   "source": [
    "py_code_tokens2 = py_tiny_solar_tokenizer.tokenize(py_code)\n",
    "print(py_code_tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   \"\"\"Find the maximum number of numbers in a list.\"\"\"\n",
      "   max = 0\n",
      "   for num in numbers:\n",
      "       if num > max:\n",
      "           max = num\n",
      "   return max\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"def find_max(numbers):\"\n",
    "\n",
    "inputs = py_tiny_solar_tokenizer(\n",
    "    prompt, return_tensors=\"pt\"\n",
    ").to(py_tiny_solar.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    py_tiny_solar_tokenizer,\n",
    "    skip_prompt=True, \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "outputs = py_tiny_solar.generate(\n",
    "    **inputs, streamer=streamer,\n",
    "    use_cache=True, \n",
    "    max_new_tokens=128, \n",
    "    do_sample=False, \n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model above has been further pretrained on python code chunks (continued pretraining), we can see that this model generates the right code that completes the given prompt, however it is still base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: We're running large models in a limited environment. Run me if you encounter any memory issues.\n",
    "import gc\n",
    "del py_tiny_solar_tokenizer\n",
    "del py_tiny_solar\n",
    "del streamer\n",
    "del outputs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Python samples with Qwen2 0.5B (Larger Instruct Model)\n",
    "\n",
    "Here you'll use a version of Qwen2 0.5B . You can find the model on Ollama at [this link](https://ollama.com/library/qwen2).\n",
    "\n",
    "You'll use Ollama to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL3 = \"qwen2:0.5b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine the maximum value in a Python list of numbers, you can use the built-in `max()` function. For example:\n",
      "\n",
      "```python\n",
      "numbers = [10, 23, 5, 78, 90]\n",
      "if len(numbers) == 0:  # Handle case where we have an empty list\n",
      "    max_number = None\n",
      "else:\n",
      "    max_number = max(numbers)\n",
      "print(max_number)\n",
      "```\n",
      "\n",
      "This code snippet defines a function called `find_max` that takes in a variable number of arguments (in this case, three), which are a list of numbers. It then checks whether the length of the list is zero or not, and handles this by returning None if so.\n",
      "\n",
      "If the list is not empty, it uses the `max()` function to find the maximum value in the list. The function returns the maximum value as an integer.\n",
      "\n",
      "The code then prints the result to the console."
     ]
    }
   ],
   "source": [
    "### Instruct model ....\n",
    "### use here ollama ...\n",
    "stream = ollama.chat(\n",
    "    model=MODEL3,\n",
    "    messages=[{'role': 'user', 'content': prompt}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this model generated a high quality response, which is more detailed that `TinySolar-248m-4k-code-instruct`, this is the effect of size, and data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "- General Pretrained models on customized tasks may perform poorly.\n",
    "- General Pretrained Models needs further pretraining on your custom data to create a good model in the future.\n",
    "- pretraining is like reading hundreds/thousands of books to learn how to generate tokens.\n",
    "- instruction tuning is like a student in the exam, the student knows how to answer the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
